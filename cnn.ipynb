{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creating a CNN for Arousal Prediction",
   "id": "92eb9dd224f4298a"
  },
  {
   "cell_type": "code",
   "id": "5b0152cfc27d0504",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Custom Dataset",
   "id": "ea1191e69e492e37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_frames=115):\n",
    "        self.data_dir = data_dir\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "        # Ottieni tutti i file di tensori\n",
    "        all_tensor_files = [f for f in os.listdir(data_dir) if f.endswith('.npy')]\n",
    "\n",
    "        # Filtra i file con abbastanza frame\n",
    "        self.tensor_files = []\n",
    "        for file_name in all_tensor_files:\n",
    "            file_path = os.path.join(data_dir, file_name)\n",
    "            tensor = np.load(file_path, mmap_mode='r')  # Usa mmap per migliorare la velocità\n",
    "            if tensor.shape[0] >= self.max_frames:\n",
    "                self.tensor_files.append(file_name)\n",
    "\n",
    "        print(f\"Loaded {len(self.tensor_files)} of {len(all_tensor_files)} files from {data_dir} (filtered out {len(all_tensor_files) - len(self.tensor_files)} files)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.tensor_files[idx]\n",
    "        file_path = os.path.join(self.data_dir, file_name)\n",
    "\n",
    "        # Carica il tensore\n",
    "        tensor = np.load(file_path, mmap_mode='r')\n",
    "        tensor = torch.tensor(tensor, dtype=torch.float32)\n",
    "\n",
    "        # Trimma se necessario\n",
    "        if tensor.shape[0] > self.max_frames:\n",
    "            tensor = tensor[:self.max_frames]    \n",
    "\n",
    "        # Estrai etichette dal nome file\n",
    "        file_parts = file_name.split('_')\n",
    "        arousal = float(file_parts[-4])\n",
    "        arousal = 0 if arousal < 5 else 1\n",
    "        labels = torch.tensor([arousal], dtype=torch.float32)\n",
    "\n",
    "        return tensor, labels, file_name\n"
   ],
   "id": "c00eed6971a9c4d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Definition of the CNN",
   "id": "ed9c818a70331a65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class EmotionCNN(pl.LightningModule):\n",
    "    def __init__(self, input_shape):\n",
    "        \"\"\"\n",
    "        input_shape: torch.Size([131, 478, 3])\n",
    "            - 131: altezza (o numero di frame)\n",
    "            - 478: larghezza (numero di landmark)\n",
    "            - 3: canali (coordinate x, y, z)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        print(f\"Model initialized with input_shape: {input_shape}\")\n",
    "\n",
    "        # Impostazioni di base\n",
    "        self.height = input_shape[0]   # 131\n",
    "        self.width = input_shape[1]    # 478\n",
    "        self.channels = input_shape[2] # 3\n",
    "\n",
    "        # Layer convoluzionali\n",
    "        self.conv1 = nn.Conv2d(in_channels=self.channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Layer Fully Connected (inizializzato con dimensioni fisse, corretto dopo)\n",
    "        self.fc1 = nn.Linear(1, 512)  # Placeholder, sarà corretto nel forward\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc_out = nn.Linear(256, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        device = x.device  # Assicura che sia su GPU o CPU in modo corretto\n",
    "\n",
    "        # Cambiamo l'ordine delle dimensioni: [batch, channels, height, width]\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # Passaggio attraverso i layer convoluzionali\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "\n",
    "        \n",
    "        fc_input_dim = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "\n",
    "        if self.fc1.in_features != fc_input_dim:\n",
    "            self.fc1 = nn.Linear(fc_input_dim, 512).to(device)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        # Layer Fully Connected\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, _ = batch\n",
    "\n",
    "        # Assicuriamoci che input e target siano sullo stesso device\n",
    "        device = self.device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Assicuriamoci che y sia float per la BCEWithLogitsLoss\n",
    "        y = y.float()\n",
    "\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n"
   ],
   "id": "cf7828e109375f4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Leave-One-Out Training",
   "id": "28989b0ab9f903de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def leave_one_out_training(data_dir, max_epochs=10):\n",
    "    users = sorted(os.listdir(data_dir))\n",
    "    all_fold_metrics = []\n",
    "\n",
    "    for i, user in enumerate(users):\n",
    "        print(f\"\\n=== Leave-One-Out Fold {i+1}/{len(users)} ===\")\n",
    "        test_user_path = os.path.join(data_dir, user, 'tensors')\n",
    "        train_users = [u for u in users if u != user]\n",
    "\n",
    "        train_datasets = []\n",
    "        for train_user in train_users:\n",
    "            user_tensors_path = os.path.join(data_dir, train_user, 'tensors')\n",
    "            if os.path.exists(user_tensors_path):\n",
    "                dataset = TensorDataset(user_tensors_path)\n",
    "                if len(dataset) > 0:\n",
    "                    train_datasets.append(dataset)\n",
    "                else:\n",
    "                    print(f\"Warning: Nessun campione valido per {train_user}\")\n",
    "\n",
    "        if not train_datasets:\n",
    "            print(f\"Skipping fold {i+1} - no training samples found\")\n",
    "            continue\n",
    "\n",
    "        train_dataset = ConcatDataset(train_datasets)\n",
    "        test_dataset = TensorDataset(test_user_path)\n",
    "        if len(test_dataset) == 0:\n",
    "            print(f\"Skipping fold {i+1} - no test samples found for user {user}\")\n",
    "            continue\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "        sample, _, _ = train_datasets[0][0]\n",
    "        input_shape = sample.shape\n",
    "        print(f\"Sample shape: {input_shape}\")\n",
    "\n",
    "        model = EmotionCNN(input_shape)\n",
    "    \n",
    "        trainer = Trainer(\n",
    "            accelerator=\"auto\",\n",
    "            devices=1,\n",
    "            max_epochs=max_epochs,\n",
    "            logger=True,\n",
    "            enable_checkpointing=True,\n",
    "            callbacks=[pl.callbacks.EarlyStopping(monitor='train_loss', patience=3)],\n",
    "            enable_progress_bar=True\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_loader) \n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (test_sample, test_label, test_file) in enumerate(test_loader):\n",
    "                print(next(model.parameters()).device)  # Dovrebbe stampare cuda:0\n",
    "                print(test_sample.device)  # Dovrebbe stampare cuda:0\n",
    "\n",
    "                print(f\"Batch {batch_idx}: test_sample shape {test_sample.shape}\")\n",
    "\n",
    "                if test_sample.nelement() == 0:\n",
    "                    print(f\"Warning: test_sample è vuoto! Salto questo batch.\")\n",
    "                    continue\n",
    "                \n",
    "                if test_sample.dim() == 3:\n",
    "                    test_sample = test_sample.unsqueeze(0).permute(0, 3, 1, 2).contiguous()\n",
    "                \n",
    "                test_pred = model(test_sample)\n",
    "\n",
    "                all_preds.append(test_pred.cpu())\n",
    "                all_labels.append(test_label.cpu())\n",
    "                print(f\"Test file: {test_file[0]}\")\n",
    "                print(f\"Prediction: {test_pred.cpu().numpy()}, True label: {test_label.cpu().numpy()}\")\n",
    "\n",
    "        if all_preds and all_labels:\n",
    "            all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "            all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "            mse = mean_squared_error(all_preds, all_labels)\n",
    "            mae = mean_absolute_error(all_preds, all_labels)\n",
    "            r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "            print(f\"Metrics for Fold {i+1}:\")\n",
    "            print(f\"  MSE: {mse:.4f}\")\n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "            print(f\"  R² Score: {r2:.4f}\")\n",
    "\n",
    "            all_fold_metrics.append({'fold': i+1, 'user': user, 'mse': mse, 'mae': mae, 'r2': r2})\n",
    "\n",
    "    if all_fold_metrics:\n",
    "        avg_mse = sum(fold['mse'] for fold in all_fold_metrics) / len(all_fold_metrics)\n",
    "        avg_mae = sum(fold['mae'] for fold in all_fold_metrics) / len(all_fold_metrics)\n",
    "        avg_r2 = sum(fold['r2'] for fold in all_fold_metrics) / len(all_fold_metrics)\n",
    "\n",
    "        print(\"\\n=== Overall Cross-Validation Results ===\")\n",
    "        print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "        print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "        print(f\"Average R² Score: {avg_r2:.4f}\")\n",
    "\n",
    "    return all_fold_metrics\n"
   ],
   "id": "3c20507a207706a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test another CNN",
   "id": "40b9877dfc1d4bd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def leave_one_out_training(data_dir, max_epochs=10):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    users = sorted(os.listdir(data_dir))\n",
    "    all_fold_metrics = []\n",
    "\n",
    "    for i, user in enumerate(users):\n",
    "        print(f\"\\n=== Leave-One-Out Fold {i+1}/{len(users)} ===\")\n",
    "        test_user_path = os.path.join(data_dir, user, 'tensors')\n",
    "        train_users = [u for u in users if u != user]\n",
    "\n",
    "        train_datasets = []\n",
    "        for train_user in train_users:\n",
    "            user_tensors_path = os.path.join(data_dir, train_user, 'tensors')\n",
    "            if os.path.exists(user_tensors_path):\n",
    "                dataset = TensorDataset(user_tensors_path)\n",
    "                if len(dataset) > 0:\n",
    "                    train_datasets.append(dataset)\n",
    "                else:\n",
    "                    print(f\"Warning: Nessun campione valido per {train_user}\")\n",
    "\n",
    "        if not train_datasets:\n",
    "            print(f\"Skipping fold {i+1} - no training samples found\")\n",
    "            continue\n",
    "\n",
    "        train_dataset = ConcatDataset(train_datasets)\n",
    "        test_dataset = TensorDataset(test_user_path)\n",
    "        if len(test_dataset) == 0:\n",
    "            print(f\"Skipping fold {i+1} - no test samples found for user {user}\")\n",
    "            continue\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "        model = Model()\n",
    "    \n",
    "        trainer = Trainer(\n",
    "            accelerator=\"auto\",\n",
    "            devices=1,\n",
    "            max_epochs=max_epochs,\n",
    "            logger=True,\n",
    "            enable_checkpointing=True,\n",
    "            callbacks=[pl.callbacks.EarlyStopping(monitor='train_loss', patience=3)],\n",
    "            enable_progress_bar=True\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, train_loader) \n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (test_sample, test_label, test_file) in enumerate(test_loader):\n",
    "                print(next(model.parameters()).device)  # Dovrebbe stampare cuda:0\n",
    "                print(test_sample.device)  # Dovrebbe stampare cuda:0\n",
    "\n",
    "                print(f\"Batch {batch_idx}: test_sample shape {test_sample.shape}\")\n",
    "\n",
    "                if test_sample.nelement() == 0:\n",
    "                    print(f\"Warning: test_sample è vuoto! Salto questo batch.\")\n",
    "                    continue\n",
    "                \n",
    "                if test_sample.dim() == 3:\n",
    "                    test_sample = test_sample.unsqueeze(0).permute(0, 3, 1, 2).contiguous()\n",
    "                \n",
    "                test_pred = model(test_sample)\n",
    "\n",
    "                all_preds.append(test_pred.cpu())\n",
    "                all_labels.append(test_label.cpu())\n",
    "                print(f\"Test file: {test_file[0]}\")\n",
    "                print(f\"Prediction: {test_pred.cpu().numpy()}, True label: {test_label.cpu().numpy()}\")\n",
    "\n",
    "        if all_preds and all_labels:\n",
    "            all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "            all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "\n",
    "            mse = mean_squared_error(all_preds, all_labels)\n",
    "            mae = mean_absolute_error(all_preds, all_labels)\n",
    "            r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "            print(f\"Metrics for Fold {i+1}:\")\n",
    "            print(f\"  MSE: {mse:.4f}\")\n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "            print(f\"  R² Score: {r2:.4f}\")\n",
    "\n",
    "            all_fold_metrics.append({'fold': i+1, 'user': user, 'mse': mse, 'mae': mae, 'r2': r2})\n",
    "\n",
    "    if all_fold_metrics:\n",
    "        avg_mse = sum(fold['mse'] for fold in all_fold_metrics) / len(all_fold_metrics)\n",
    "        avg_mae = sum(fold['mae'] for fold in all_fold_metrics) / len(all_fold_metrics)\n",
    "        avg_r2 = sum(fold['r2'] for fold in all_fold_metrics) / len(all_fold_metrics)\n",
    "\n",
    "        print(\"\\n=== Overall Cross-Validation Results ===\")\n",
    "        print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "        print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "        print(f\"Average R² Score: {avg_r2:.4f}\")\n",
    "\n",
    "    return all_fold_metrics\n"
   ],
   "id": "f9a36ac353385cb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Start the Leave-One-Out Training",
   "id": "bba2753d1bacdc8e"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "data_dir = \"data\"\n",
    "leave_one_out_training(data_dir, max_epochs=10)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_tensor_dimensions_for_all_users(base_dir, output_csv='tensor_dimensions_all_users.csv'):\n",
    "    tensor_data = []\n",
    "\n",
    "    # Itera su tutti gli utenti\n",
    "    for user in os.listdir(base_dir):\n",
    "        user_dir = os.path.join(base_dir, user)\n",
    "        if not os.path.isdir(user_dir):  # Ignora se non è una cartella\n",
    "            continue\n",
    "        \n",
    "        # Trova la cartella \"tensors\" all'interno di ogni utente\n",
    "        tensors_dir = os.path.join(user_dir, 'tensors')\n",
    "        if not os.path.isdir(tensors_dir):\n",
    "            continue\n",
    "\n",
    "        # Lista tutti i file .npy\n",
    "        tensor_files = [f for f in os.listdir(tensors_dir) if f.endswith('.npy')]\n",
    "\n",
    "        for file_name in tensor_files:\n",
    "            file_path = os.path.join(tensors_dir, file_name)\n",
    "            \n",
    "            # Carica il tensore\n",
    "            tensor = np.load(file_path)\n",
    "            \n",
    "            # Ottieni la forma del tensore (numero di frame, landmarks, 3D)\n",
    "            tensor_shape = tensor.shape\n",
    "            \n",
    "            # Aggiungi i dati al dataframe\n",
    "            tensor_data.append({\n",
    "                'user': user,\n",
    "                'file_name': file_name,\n",
    "                'num_frames': tensor_shape[0],\n",
    "                'num_landmarks': tensor_shape[1],\n",
    "                'num_coordinates': tensor_shape[2]\n",
    "            })\n",
    "            \n",
    "            print(f\"User: {user}, File: {file_name}, Shape: {tensor_shape}\")\n",
    "\n",
    "    # Crea un DataFrame con i dati raccolti\n",
    "    df = pd.DataFrame(tensor_data)\n",
    "    \n",
    "    # Salva il DataFrame in un file CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"\\n--- Dati salvati in {output_csv} ---\")\n",
    "    print(df.head())\n",
    "\n",
    "# Esegui l'analisi su tutti gli utenti\n",
    "base_dir = \"data\"\n",
    "analyze_tensor_dimensions_for_all_users(base_dir)\n"
   ],
   "id": "5c73cfa2da81e2c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#analyze the balance of the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_dataset_balance(data_dir):\n",
    "    tensor_data = []\n",
    "\n",
    "    # Itera su tutti gli utenti\n",
    "    for user in os.listdir(data_dir):\n",
    "        user_dir = os.path.join(data_dir, user)\n",
    "        if not os.path.isdir(user_dir):  # Ignora se non è una cartella\n",
    "            continue\n",
    "        \n",
    "        # Trova la cartella \"tensors\" all'interno di ogni utente\n",
    "        tensors_dir = os.path.join(user_dir, 'tensors')\n",
    "        if not os.path.isdir(tensors_dir):\n",
    "            continue\n",
    "\n",
    "        for f in os.listdir(tensors_dir):\n",
    "            if f.endswith('.npy'):\n",
    "                tensor_data.append({\n",
    "                    'user': user,\n",
    "                    'file_name': f,\n",
    "                    'arousal': 1 if float(f.split('_')[-4]) > 5 else 0\n",
    "                })\n",
    "                \n",
    "    # Crea un DataFrame con i dati raccolti\n",
    "    df = pd.DataFrame(tensor_data)\n",
    "    \n",
    "    # Conta i valori unici\n",
    "    counts = df['arousal'].value_counts()\n",
    "    print(counts)\n",
    "\n",
    "analyze_dataset_balance(\"data\")    \n",
    "    \n",
    "    "
   ],
   "id": "198e251c70d30504",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
